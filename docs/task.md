
## 任务记录

### 9.5
- [x] 利用SAC算法训练一个agent，并且提交到在线系统;
    - 结果：线上15.3， 线下20.0

### 9.12     
- [x] 整理目前使用的动作空间和状态空间
    - [文档可见](https://github.com/mikezhang95/grid_control/blob/main/docs/env.md#%E5%90%91%E9%87%8F%E5%8C%96observation)
    - 动作空间对于结果更加敏感
    - 现在的电压动作只能增加，是不合理的

- [x] 尝试Imitation Learning
    - 从表格读取第一个时刻的gen_p，作为label
    - gen_v随机生成，作为label
    - 结果：效果无提升
    - 原因：只有第一时刻的数据，样本数量、多样性不足
    
### 9.19

- [x] 参加赛道研讨会

- [x] 合理化Action Space
    - action只训练gen_p, fix gen_v=0
    - 结果：效果提升非常明显，线上从15 -> 42
    - gen_p scale扩大10倍
    - 结果：效果提升非常明显，线上从42 -> 207

### 9.30

- [x] Stable-baselines3 环境优化
    * FrameStack: 和不使用Stack相比没有明显提升
    * 多进程: 只有PPO可以使用多进程训练，没有比SAC提升
    * 特征归一化 + 奖励归一化: 没有明显提升，原因待确认 TODO

- [x] RL算法优化，包括调参、使用其他算法
    * SAC > PPO > TD3

- [x] Action Space优化，包括在训练gen_p的模型上微调gen_v
    * 固定gen_p的策略，只调整gen_v，效果雪崩

### 10.7

- [ ] 增加规则，避免违反电网规则 / 初始化选择断面


- [x] Observation Space优化，包括特征归一化、特征选择、Graph Learning
    * 使用和paper里一样的状态空间，维度更少 // 效果下降了
    * 使用Stable-baselines3提供的动态归一化工具VecNormalize，而不是手动归一化  // 效果不怎么变，而且测试的时候无法还原
 
- [x] 环境增加噪声，提高鲁棒性 // 效果不明显



## 实验结果

| model_name    | 训练结果 | 线上结果 | 
|---------------|---------|--------|
|  sac_trial    |  20.0   |  15    |
|  sac_genp_t1  |  207.2  |  55    |
|  sac_genp_t10 |  1613.8 |  258   | 

[【腾讯文档】电网比赛实验结果](https://docs.qq.com/sheet/DYWNnSXJNSlVhS09G) 

## QAs:

Q1: 潮流前，潮流后什么意思              
A. PowerFlow计算收敛之前和之后的状态

Q2: grid_loss 是怎么计算的，每一步只有一个值？             
A: 传输线路上的耗损

Q3: 断面          
A: == step

Q4: forecast reader给了后一时刻的信息                
A: 通过预测下一时刻的信息更好的控制机器，之类直接给出下一时刻的真实负载和新能源发电信息

Q5: legal act space 是怎么计算出来的                    
A: 通过爬坡功率和发电机的最大最小功率一起计算得到


## Take-away

1. 尽早建立训练、评估RL模型的基本框架，包括
* 多进程、多线程从环境采样、更新模型，提高训练效率
* 评估模型使用统一的奖励机制(归一化前后、稠密化前后)，多次采样取得一个可靠的结果 (电网环境波动很大)
* 模仿学习等框架

2. 按照优先级改进RL算法
* 状态空间、动作空间对于结果影响应该是最大的，应该优先确定 // 但是缺少一种方法论，找到最优的状态特征，和特征处理方法
* 奖励空间，这次没有处理，待研究
* 选择合适的RL算法(SAC效果不错)
* 网络结构、RL算法参数对于结果影响比较微弱，可以最后微调

3. 专家知识是非常重要的
* 了解清楚状态空间动作空间的实际含义
* RL算法无法学习到一些corner case，或者对于约束的处理不好
* 在调试的时候，有专家知识会更快定位问题
* 可以先构建一个规则的模型，方便对比和模仿学习等
* 和RL模型融合，形成更强大的策略



